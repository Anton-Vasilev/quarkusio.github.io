<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This guide explains how you can send your logs to a centralized log management system like Graylog, Logstash (inside the Elastic Stack or ELK - Elasticsearch, Logstash, Kibana) or
Fluentd (inside EFK - Elasticsearch, Fluentd, Kibana).</p>
</div>
<div class="paragraph">
<p>There are a lot of different ways to centralize your logs (if you are using Kubernetes, the simplest way is to log to the console and ask you cluster administrator to integrate a central log manager inside your cluster).
In this guide, we will expose how to send them to an external tool using the <code>quarkus-logging-gelf</code> extension that can use TCP or UDP to send logs in the Graylog Extended Log Format (GELF).</p>
</div>
<div class="paragraph">
<p>The <code>quarkus-logging-gelf</code> extension will add a GELF log handler to the underlying logging backend that Quarkus uses (jboss-logmanager).
By default, it is disabled, if you enable it but still use another handler (by default the console handler is enabled), your logs will be sent to both handlers.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="prerequisites"><a class="anchor" href="#prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Unresolved directive in centralized-log-management.adoc - include::{includes}/prerequisites.adoc[]</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="example-application"><a class="anchor" href="#example-application"></a>Example application</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following examples will all be based on the same example application that you can create with the following steps.</p>
</div>
<div class="paragraph">
<p>Create an application with the <code>quarkus-logging-gelf</code> extension. You can use the following command to create it:</p>
</div>
<div class="paragraph">
<p>Unresolved directive in centralized-log-management.adoc - include::{includes}/devtools/create-app.adoc[]</p>
</div>
<div class="paragraph">
<p>If you already have your Quarkus project configured, you can add the <code>logging-gelf</code> extension
to your project by running the following command in your project base directory:</p>
</div>
<div class="paragraph">
<p>Unresolved directive in centralized-log-management.adoc - include::{includes}/devtools/extension-add.adoc[]</p>
</div>
<div class="paragraph">
<p>This will add the following dependency to your build file:</p>
</div>
<div class="listingblock primary asciidoc-tabs-target-sync-cli asciidoc-tabs-target-sync-maven">
<div class="title">pom.xml</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-xml hljs" data-lang="xml">&lt;dependency&gt;
    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;
    &lt;artifactId&gt;quarkus-logging-gelf&lt;/artifactId&gt;
&lt;/dependency&gt;</code></pre>
</div>
</div>
<div class="listingblock secondary asciidoc-tabs-target-sync-gradle">
<div class="title">build.gradle</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-gradle hljs" data-lang="gradle">implementation("io.quarkus:quarkus-logging-gelf")</code></pre>
</div>
</div>
<div class="paragraph">
<p>For demonstration purposes, we create an endpoint that does nothing but log a sentence. You don&#8217;t need to do this inside your application.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">import jakarta.enterprise.context.ApplicationScoped;
import jakarta.ws.rs.GET;
import jakarta.ws.rs.Path;

import org.jboss.logging.Logger;

@Path("/gelf-logging")
@ApplicationScoped
public class GelfLoggingResource {
    private static final Logger LOG = Logger.getLogger(GelfLoggingResource.class);

    @GET
    public void log() {
        LOG.info("Some useful log message");
    }

}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Configure the GELF log handler to send logs to an external UDP endpoint on the port 12201:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-properties hljs" data-lang="properties">quarkus.log.handler.gelf.enabled=true
quarkus.log.handler.gelf.host=localhost
quarkus.log.handler.gelf.port=12201</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="send-logs-to-graylog"><a class="anchor" href="#send-logs-to-graylog"></a>Send logs to Graylog</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To send logs to Graylog, you first need to launch the components that compose the Graylog stack:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>MongoDB</p>
</li>
<li>
<p>Elasticsearch</p>
</li>
<li>
<p>Graylog</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can do this via the following <code>docker-compose.yml</code> file that you can launch via <code>docker-compose up -d</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">version: '3.2'

services:
  elasticsearch:
    image: docker.io/elastic/elasticsearch:7.16.3
    ports:
      - "9200:9200"
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      discovery.type: "single-node"
    networks:
      - graylog

  mongo:
    image: mongo:4.0
    networks:
      - graylog

  graylog:
    image: graylog/graylog:4.3.0
    ports:
      - "9000:9000"
      - "12201:12201/udp"
      - "1514:1514"
    environment:
      GRAYLOG_HTTP_EXTERNAL_URI: "http://127.0.0.1:9000/"
      # CHANGE ME (must be at least 16 characters)!
      GRAYLOG_PASSWORD_SECRET: "forpasswordencryption"
      # Password: admin
      GRAYLOG_ROOT_PASSWORD_SHA2: "8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918"
    networks:
      - graylog
    depends_on:
      - elasticsearch
      - mongo

networks:
  graylog:
    driver: bridge</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then, you need to create a UDP input in Graylog.
You can do it from the Graylog web console (System &#8594; Input &#8594; Select GELF UDP) available at <a href="http://localhost:9000" class="bare">http://localhost:9000</a> or via the API.</p>
</div>
<div class="paragraph">
<p>This curl example will create a new Input of type GELF UDP, it uses the default login from Graylog (admin/admin).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">curl -H "Content-Type: application/json" -H "Authorization: Basic YWRtaW46YWRtaW4=" -H "X-Requested-By: curl" -X POST -v -d \
'{"title":"udp input","configuration":{"recv_buffer_size":262144,"bind_address":"0.0.0.0","port":12201,"decompress_size_limit":8388608},"type":"org.graylog2.inputs.gelf.udp.GELFUDPInput","global":true}' \
http://localhost:9000/api/system/inputs</code></pre>
</div>
</div>
<div class="paragraph">
<p>Launch your application, you should see your logs arriving inside Graylog.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="send-logs-to-logstash-the-elastic-stack-elk"><a class="anchor" href="#send-logs-to-logstash-the-elastic-stack-elk"></a>Send logs to Logstash / the Elastic Stack (ELK)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Logstash comes by default with an Input plugin that can understand the GELF format, we will first create a pipeline that enables this plugin.</p>
</div>
<div class="paragraph">
<p>Create the following file  in <code>$HOME/pipelines/gelf.conf</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">input {
  gelf {
    port =&gt; 12201
  }
}
output {
  stdout {}
  elasticsearch {
    hosts =&gt; ["http://elasticsearch:9200"]
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, launch the components that compose the Elastic Stack:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Elasticsearch</p>
</li>
<li>
<p>Logstash</p>
</li>
<li>
<p>Kibana</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can do this via the following <code>docker-compose.yml</code> file that you can launch via <code>docker-compose up -d</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Launch Elasticsearch
version: '3.2'

services:
  elasticsearch:
    image: docker.io/elastic/elasticsearch:7.16.3
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      discovery.type: "single-node"
    networks:
      - elk

  logstash:
    image: docker.io/elastic/logstash:7.16.3
    volumes:
      - source: $HOME/pipelines
        target: /usr/share/logstash/pipeline
        type: bind
    ports:
      - "12201:12201/udp"
      - "5000:5000"
      - "9600:9600"
    networks:
      - elk
    depends_on:
      - elasticsearch

  kibana:
    image: docker.io/elastic/kibana:7.16.3
    ports:
      - "5601:5601"
    networks:
      - elk
    depends_on:
      - elasticsearch

networks:
  elk:
    driver: bridge</code></pre>
</div>
</div>
<div class="paragraph">
<p>Launch your application, you should see your logs arriving inside the Elastic Stack; you can use Kibana available at <a href="http://localhost:5601/" class="bare">http://localhost:5601/</a> to access them.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="send-logs-to-fluentd-efk"><a class="anchor" href="#send-logs-to-fluentd-efk"></a>Send logs to Fluentd (EFK)</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, you need to create a Fluentd image with the needed plugins: elasticsearch and input-gelf.
You can use the following Dockerfile that should be created inside a <code>fluentd</code> directory.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-dockerfile hljs" data-lang="dockerfile">FROM fluent/fluentd:v1.3-debian
RUN ["gem", "install", "fluent-plugin-elasticsearch", "--version", "3.7.0"]
RUN ["gem", "install", "fluent-plugin-input-gelf", "--version", "0.3.1"]</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can build the image or let docker-compose build it for you.</p>
</div>
<div class="paragraph">
<p>Then you need to create a fluentd configuration file inside <code>$HOME/fluentd/fluent.conf</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">&lt;source&gt;
  type gelf
  tag example.gelf
  bind 0.0.0.0
  port 12201
&lt;/source&gt;

&lt;match example.gelf&gt;
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
&lt;/match&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, launch the components that compose the EFK Stack:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Elasticsearch</p>
</li>
<li>
<p>Fluentd</p>
</li>
<li>
<p>Kibana</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can do this via the following <code>docker-compose.yml</code> file that you can launch via <code>docker-compose up -d</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">version: '3.2'

services:
  elasticsearch:
    image: docker.io/elastic/elasticsearch:7.16.3
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      discovery.type: "single-node"
    networks:
      - efk

  fluentd:
    build: fluentd
    ports:
      - "12201:12201/udp"
    volumes:
      - source: $HOME/fluentd
        target: /fluentd/etc
        type: bind
    networks:
      - efk
    depends_on:
      - elasticsearch

  kibana:
    image: docker.io/elastic/kibana:7.16.3
    ports:
      - "5601:5601"
    networks:
      - efk
    depends_on:
      - elasticsearch

networks:
  efk:
    driver: bridge</code></pre>
</div>
</div>
<div class="paragraph">
<p>Launch your application, you should see your logs arriving inside EFK: you can use Kibana available at <a href="http://localhost:5601/" class="bare">http://localhost:5601/</a> to access them.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="gelf-alternative-use-syslog"><a class="anchor" href="#gelf-alternative-use-syslog"></a>GELF alternative: use Syslog</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can also send your logs to Fluentd using a Syslog input.
As opposed to the GELF input, the Syslog input will not render multiline logs in one event, that&#8217;s why we advise to use the GELF input that we implement in Quarkus.</p>
</div>
<div class="paragraph">
<p>First, you need to create a Fluentd image with the elasticsearch plugin.
You can use the following Dockerfile that should be created inside a <code>fluentd</code> directory.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-dockerfile hljs" data-lang="dockerfile">FROM fluent/fluentd:v1.3-debian
RUN ["gem", "install", "fluent-plugin-elasticsearch", "--version", "3.7.0"]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then, you need to create a fluentd configuration file inside <code>$HOME/fluentd/fluent.conf</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-none hljs">&lt;source&gt;
  @type syslog
  port 5140
  bind 0.0.0.0
  message_format rfc5424
  tag system
&lt;/source&gt;

&lt;match **&gt;
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
&lt;/match&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then, launch the components that compose the EFK Stack:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Elasticsearch</p>
</li>
<li>
<p>Fluentd</p>
</li>
<li>
<p>Kibana</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can do this via the following <code>docker-compose.yml</code> file that you can launch via <code>docker-compose up -d</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">version: '3.2'

services:
  elasticsearch:
    image: docker.io/elastic/elasticsearch:7.16.3
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
    networks:
      - efk

  fluentd:
    build: fluentd
    ports:
      - "5140:5140/udp"
    volumes:
      - source: $HOME/fluentd
        target: /fluentd/etc
        type: bind
    networks:
      - efk
    depends_on:
      - elasticsearch

  kibana:
    image: docker.io/elastic/kibana:7.16.3
    ports:
      - "5601:5601"
    networks:
      - efk
    depends_on:
      - elasticsearch

networks:
  efk:
    driver: bridge</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, configure your application to send logs to EFK using Syslog:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-properties hljs" data-lang="properties">quarkus.log.syslog.enable=true
quarkus.log.syslog.endpoint=localhost:5140
quarkus.log.syslog.protocol=udp
quarkus.log.syslog.app-name=quarkus
quarkus.log.syslog.hostname=quarkus-test</code></pre>
</div>
</div>
<div class="paragraph">
<p>Launch your application, you should see your logs arriving inside EFK: you can use Kibana available at <a href="http://localhost:5601/" class="bare">http://localhost:5601/</a> to access them.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="elasticsearch-indexing-consideration"><a class="anchor" href="#elasticsearch-indexing-consideration"></a>Elasticsearch indexing consideration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Be careful that, by default, Elasticsearch will automatically map unknown fields (if not disabled in the index settings) by detecting their type.
This can become tricky if you use log parameters (which are included by default), or if you enable MDC inclusion (disabled by default),
as the first log will define the type of the message parameter (or MDC parameter) field inside the index.</p>
</div>
<div class="paragraph">
<p>Imagine the following case:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-java hljs" data-lang="java">LOG.info("some {} message {} with {} param", 1, 2, 3);
LOG.info("other {} message {} with {} param", true, true, true);</code></pre>
</div>
</div>
<div class="paragraph">
<p>With log message parameters enabled, the first log message sent to Elasticsearch will have a <code>MessageParam0</code> parameter with an <code>int</code> type;
this will configure the index with a field of type <code>integer</code>.
When the second message will arrive to Elasticsearch, it will have a <code>MessageParam0</code> parameter with the boolean value <code>true</code>, and this will generate an indexing error.</p>
</div>
<div class="paragraph">
<p>To work around this limitation, you can disable sending log message parameters via <code>logging-gelf</code> by configuring <code>quarkus.log.handler.gelf.include-log-message-parameters=false</code>,
or you can configure your Elasticsearch index to store those fields as text or keyword, Elasticsearch will then automatically make the translation from int/boolean to a String.</p>
</div>
<div class="paragraph">
<p>See the following documentation for Graylog (but the same issue exists for the other central logging stacks): <a href="https://docs.graylog.org/en/3.2/pages/configuration/elasticsearch.html#custom-index-mappings">Custom Index Mappings</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="configuration-reference"><a class="anchor" href="#configuration-reference"></a>Configuration Reference</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Configuration is done through the usual <code>application.properties</code> file.</p>
</div>
<div class="paragraph">
<p>This extension uses the <code>logstash-gelf</code> library that allow more configuration options via system properties,
you can access its documentation here: <a href="https://logging.paluch.biz/" class="bare">https://logging.paluch.biz/</a> .</p>
</div>
</div>
</div>